{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Liar_preprocessing.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1hxT7QioKgeOEpMSeKois0anNKYECCtlm","authorship_tag":"ABX9TyMsEl5/+IHufYrcGDj2/XdE"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"cApkniXCMTFJ","colab_type":"code","colab":{}},"source":["import re\n","import nltk\n","# nltk.download('punkt')\n","# nltk.download('stopwords')\n","# nltk.download('averaged_perceptron_tagger')\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.corpus import stopwords\n","from nltk.stem.snowball import SnowballStemmer\n","from collections import Counter\n","# !pip install vaderSentiment\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n","import pandas as pd\n","# !pip install empath\n","from empath import Empath"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aSIXFJbhb8NV","colab_type":"code","colab":{}},"source":["def cleanText(text):\n","  # removing IP addresses from text\n","  text = re.sub(r'[0-9]+(?:\\.[0-9]+){3}', '', text, flags=re.MULTILINE)\n","\n","  # removing URLs from text\n","  text = re.sub(r'http\\S+', '', text, flags=re.MULTILINE)\n","\n","  # remove punctuations and creating text tokens\n","  nltk_tokenizer = RegexpTokenizer(r'\\w+')\n","  text_tokens = nltk_tokenizer.tokenize(text)\n","\n","  # removing stop words\n","  tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n","\n","  # stemming using snowball stemmer\n","  stem_tokens =[]\n","  stemmer = SnowballStemmer(\"english\")\n","  for token in tokens_without_sw:\n","      token = stemmer.stem(token)\n","      if token != \"\":\n","        stem_tokens.append(token)\n","\n","  # joining sentences\n","  preprocessed_text = ' '.join(word for word in stem_tokens)\n","  return preprocessed_text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BHRnMgrnspZK","colab_type":"code","outputId":"b2a97c8b-2e48-41d4-d33a-8cb00f81d648","executionInfo":{"status":"ok","timestamp":1591649643170,"user_tz":-300,"elapsed":864,"user":{"displayName":"azka kishwar","photoUrl":"","userId":"06448903578955076563"}},"colab":{"base_uri":"https://localhost:8080/","height":117}},"source":["lexicon = Empath()\n","lexicon.analyze(text, categories=[\"violence\",\"crime\",\"pride\",\"sympathy\",\"deception\",\"war\"])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'crime': 0.0,\n"," 'deception': 0.0,\n"," 'pride': 0.0,\n"," 'sympathy': 0.0,\n"," 'violence': 1.0,\n"," 'war': 0.0}"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"t_HvU93Hb6HG","colab_type":"code","colab":{}},"source":["# Lexical feature extraction\n","\n","# length of word\n","def articleLength(text):\n","  return len(text)\n","\n","# average word length\n","def averageWordLength(text):\n","  words = text.split()\n","  average = sum(len(word) for word in words) / len(words)\n","  return int(average)\n","\n","# count of numbers\n","def countOfNumbers(text):\n","  count_of_numbers = len(\"\".join(re.findall(\"\\d+\", text)))\n","  return count_of_numbers\n","\n","#count of exclaimation marks\n","def countOfExclaimationMarks(text):\n","  count_of_em = len(\"\".join(re.findall(\"!+\", text)))\n","  return count_of_em \n","\n","# get tokens of text\n","def textTokens(text):\n","  return nltk.word_tokenize(text)\n","\n","# count of adjectives\n","def countOfAdjectives(tokens):\n","  tags = nltk.pos_tag(tokens)\n","  tag_count = Counter( tag for word,  tag in tags)\n","  count_of_adjectives = tag_count['JJ']+tag_count['JJR']+tag_count['JJS'] \n","  return count_of_adjectives\n","\n","# word count\n","def wordCount(tokens):\n","  # removing punctuation marks\n","  punct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n","  filtered_tokens = [w for w in tokens if punct.match(w)]\n","  return len(filtered_tokens)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_K5NPxb3cONX","colab_type":"code","colab":{}},"source":["# sentimental features\n","def getSentimentValues(text):\n","  sid_obj = SentimentIntensityAnalyzer() \n","  sentiment_dict = sid_obj.polarity_scores(text)\n","  return sentiment_dict"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XxQJBctwoT9W","colab_type":"code","colab":{}},"source":["# get label value for Liar\n","def getLabelLiar(label):\n","  label = label.lower()\n","  neg_labels = [\"pants-fire\",\"false\",\"barely-true\"]\n","  pos_labels = [\"half-true\",\"mostly-true\",\"true\"]\n","  if label in neg_labels:\n","    return \"FALSE\"\n","  elif label in pos_labels:\n","    return \"TRUE\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EbMPSk9QE3P9","colab_type":"code","colab":{}},"source":["# get label value for Real and Fake\n","def getLabelRealAndFake(label):\n","  label = label.lower()\n","  if label == \"fake\":\n","    return \"FALSE\"\n","  elif label == \"real\":\n","    return \"TRUE\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q0080YYE3WWY","colab_type":"code","colab":{}},"source":["# Feature Extraction\n","def featureExtraction(dataset,dataset_name):\n","  # initializing columns as lists\n","  article=[]\n","  cleaned_text=[]\n","  article_length=[]\n","  avg_word_length=[]\n","  count_of_numbers=[]\n","  count_of_em=[]\n","  count_of_adj =[]\n","  word_count = []\n","  neg_sent=[]\n","  pos_sent=[]\n","  neut_sent=[]\n","  text_label=[]\n","\n","  # setting value of each column by iterating over dataset\n","  for row in dataset:\n","    # lexical features\n","    text = row['statement']\n","    article.append(text)  #text of statement\n","    cleaned_text.append(cleanText(text))  #cleaned text of statement\n","    article_length.append(articleLength(text)) #length of article/statement\n","    avg_word_length.append(averageWordLength(text)) #average length of word\n","    count_of_numbers.append(countOfNumbers(text)) #count of numbers in text\n","    count_of_em.append(countOfExclaimationMarks(text)) #count of exclaimation marks in text\n","    text_tokens = textTokens(text) #tokens of text\n","    count_of_adj.append(countOfAdjectives(text_tokens))  #count of adjectives in text\n","    word_count.append(wordCount(text_tokens))  #word count in text\n","    \n","    # sentmiment features\n","    text_sentiment = getSentimentValues(text)  #get positive negative and neutral sentiment values of text\n","    neg_sent.append(text_sentiment['neg'])\n","    pos_sent.append(text_sentiment['pos'])\n","    neut_sent.append(text_sentiment['neu'])\n","\n","    # label of text\n","    if dataset_name == \"liar\":\n","      text_label.append(getLabelLiar(row['label']))\n","    else:\n","      text_label.append(getLabelRealAndFake(row['label']))\n","    # break\n","\n","  # setting the whole dataframe with columns\n","  features = pd.DataFrame({\"Statement\":article,\n","                           \"Cleaned Statement\":cleaned_text,\n","                          \"Article Length\":article_length,\n","                            \"Average Word Length\":avg_word_length,\n","                            \"Count of Numbers\":count_of_numbers,\n","                            \"Count of Exclaimation Marks\":count_of_em,\n","                            \"Count of Adjectives\":count_of_adj,\n","                            \"Word Count\":word_count,\n","                            \"Negative Sentiment\": neg_sent,\n","                            \"Positive Sentiment\": pos_sent,\n","                            \"Neutral Sentiment\": neut_sent,\n","                        \"Label\": text_label})\n","  return features"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cLmm1rGu4ENa","colab_type":"code","colab":{}},"source":["# create excel using dataframe\n","def createCSV(data,path):\n","  data.to_csv(path,index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hnfnILpRoIP4","colab_type":"code","outputId":"a9caa083-aff0-4d7f-f188-87e1982fced4","executionInfo":{"status":"ok","timestamp":1592055000959,"user_tz":-300,"elapsed":909142,"user":{"displayName":"azka kishwar","photoUrl":"","userId":"06448903578955076563"}},"colab":{"base_uri":"https://localhost:8080/","height":117}},"source":["# Feature extraction of LIAR dataset\n","files = ['train.tsv','test.tsv','valid.tsv']\n","feature_files = ['liar_train_features.csv','liar_test_features.csv','liar_validation_features.csv']\n","main_path = '/content/drive/My Drive/Fake news detection/preprocessing/liar/'\n","for i in range(len(files)):\n","  # read liar file\n","  dataset = pd.read_csv(main_path+files[i],delimiter='\\t',encoding='utf-8', usecols=[1,2], header=0).to_dict(orient='records')\n","  # extract features\n","  liar_features = featureExtraction(dataset,'liar') \n","  # create csv of features\n","  createCSV(liar_features,main_path+feature_files[i])\n","  print(\"\\n \"+files[i]+\" done\")  \n","                  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n"," train.tsv done\n","\n"," test.tsv done\n","\n"," valid.tsv done\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EGIkF9dhJpzE","colab_type":"code","outputId":"b3ec9aa6-d9ec-466a-f7d7-cb4241e0cf4e","executionInfo":{"status":"ok","timestamp":1592210273707,"user_tz":-300,"elapsed":1208,"user":{"displayName":"azka kishwar","photoUrl":"","userId":"06448903578955076563"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["# %%timeit\n","# read dataset \n","main_path = '/content/drive/My Drive/Fake news detection/preprocessing/real_and_fake/'\n","real_and_fake = pd.read_csv(main_path+'fake_or_real_news.csv', usecols=[2,3], header=0)\n","\n","# only select stories with lengths gt 1 -- there are some texts with len = 0 and 1\n","mask = list(real_and_fake['text'].apply(lambda x: len(x) > 1))\n","real_and_fake = real_and_fake[mask]\n","\n","# rename column text to statements\n","real_and_fake.rename(columns={'text':'statement'},inplace=True)\n","print('Found %s texts.' %real_and_fake['statement'].shape[0])\n","\n","# change to data to dictionary\n","real_and_fake = real_and_fake.to_dict(orient='records')\n","\n","# extract features\n","features = featureExtraction(real_and_fake,'real_and_fake') \n","# create csv of features\n","createCSV(features,main_path+'fake_or_real_news_features.csv')\n"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Found 6299 texts.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8_6L3R8mHsRz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"ce15ed6a-c80a-4c4b-f7c4-d5491e937bfd","executionInfo":{"status":"ok","timestamp":1592210277077,"user_tz":-300,"elapsed":1122,"user":{"displayName":"azka kishwar","photoUrl":"","userId":"06448903578955076563"}}},"source":["all_labels=[]\n","for row in real_and_fake:\n","  text_label = row['label']\n","  all_labels.append(getLabelRealAndFake(text_label))\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["6299\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6Np0PQLiIPFq","colab_type":"code","colab":{}},"source":["df = pd.read_csv(main_path+'fake_or_real_news_features_original.csv')\n","df[\"Label\"] = all_labels\n","df.to_csv(main_path+'fake_or_real_news_features_original_with_label.csv', index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fILEc7dfEFnu","colab_type":"code","outputId":"712c22b7-7e2a-43b1-9357-6ebeb3620f68","executionInfo":{"status":"ok","timestamp":1592181646467,"user_tz":-300,"elapsed":5473,"user":{"displayName":"azka kishwar","photoUrl":"","userId":"06448903578955076563"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["# https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/01.07-Timing-and-Profiling.ipynb#scrollTo=FxQa7OOGDIH5\n","# to get time of the running cell\n","%%timeit\n","# to get time of single command\n","%timeit"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The slowest run took 23.02 times longer than the fastest. This could mean that an intermediate result is being cached.\n","100000 loops, best of 3: 10.9 Âµs per loop\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2oadqxc4GJzE","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}